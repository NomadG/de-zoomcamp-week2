# de-zoomcamp-week2

Homework submission for **Week 02: Workflow Orchestration**.

This repository contains:
- Answers to all homework questions
- Kestra YAML flow definitions inside the `flows/` directory
- docker-compose.yaml for running the Kestra container

---

## Question 1

**Question:**  
Within the execution for Yellow Taxi data for the year 2020 and month 12, what is the uncompressed file size of the output file `yellow_tripdata_2020-12.csv` generated by the extract task?

**Answer:**  
**134.5 MiB**

---

## Question 2

**Question:**  
What is the rendered value of the variable `file` when:
- taxi = green  
- year = 2020  
- month = 04  

Given the template:
```

{{inputs.taxi}}*tripdata*{{inputs.year}}-{{inputs.month}}.csv

````

**Answer:**  
**green_tripdata_2020-04.csv**

---

## Question 3

**Question:**  
How many rows are there for the Yellow Taxi data across all CSV files for the year 2020?

**Answer:**  
**24,648,499**

**SQL Query:**
```sql
SELECT COUNT(*)
FROM `premium-buckeye-484913-s0.kestra_homework_dataset.yellow_tripdata`
WHERE LOWER(filename) LIKE 'yellow_tripdata_2020%';
````

---

## Question 4

**Question:**
How many rows are there for the Green Taxi data across all CSV files for the year 2020?

**Answer:**
**1,734,051**

**SQL Query:**

```sql
SELECT COUNT(*)
FROM `premium-buckeye-484913-s0.kestra_homework_dataset.green_tripdata`
WHERE LOWER(filename) LIKE 'green_tripdata_2020%';
```

---

## Question 5

**Question:**
How many rows are there for the Yellow Taxi data for the March 2021 CSV file?

**Answer:**
**1,925,152**

**SQL Query:**

```sql
SELECT COUNT(*)
FROM `premium-buckeye-484913-s0.kestra_homework_dataset.yellow_tripdata`
WHERE LOWER(filename) LIKE 'yellow_tripdata_2021-03%';
```

---

## Question 6

**Question:**
How would you configure the timezone to New York in a Schedule trigger?

**Answer:**
Add a `timezone` property set to `America/New_York` in the Schedule trigger configuration.

---

## Notes

* All workflows are implemented using **Kestra**
* YAML flow definitions are located in the `flows/` directory
* Data ingestion targets **Google Cloud Storage** and **BigQuery**
* Dataset ingestion is orchestrated using **ForEach loops** combined with **Subflows** to process multiple taxi types and months
* Queries were executed on BigQuery after successful pipeline execution

```